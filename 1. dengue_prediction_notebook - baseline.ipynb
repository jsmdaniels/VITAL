{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import precision_recall_fscore_support as get_metrics\n",
    "from sklearn.metrics import roc_auc_score as get_auc\n",
    "\n",
    "import pipeline_blocks.core_pipeline_function_wrappers\n",
    "from pipeline_blocks.core_pipeline_function_wrappers import generate_unusable_data_lists\n",
    "from pipeline_blocks.core_pipeline_function_wrappers import generate_and_reduce_patient_matrix\n",
    "#from pipeline_blocks.core_pipeline_function_wrappers import generate_features\n",
    "from utils.pickle_functions import pickle_already_exists, load_pickle\n",
    "from utils.helpers import print_num_patients_with_SC, extract_patients\n",
    "\n",
    "from pipeline_blocks.event_matching import print_patient_matrix\n",
    "from pipeline_blocks.model_training import get_train_and_val_data\n",
    "from pipeline_blocks.model_training import create_and_compile_model, get_binary_class_occurrences, train_and_evaluate_model, plot_model_histories, preprocess_stft, remove_nan_values_from_dataset, print_class_balance\n",
    "from pipeline_blocks.feature_extraction import plot_spectrograms\n",
    "\n",
    "from main_controller import load_or_create_pickle\n",
    "from importlib import reload"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There are some path issues, make sure the root_path is pointing to the root of \"dengue-severity-classification\" in order for the rest of the notebook to work\n",
    "#os.chdir('..')\n",
    "root_path = os.path.dirname(os.getcwd())\n",
    "#root_path = ''\n",
    "print(root_path)\n",
    "#Define path to 01NVa dataset (point to Imperial RDS drive mounted on your college PC)\n",
    "base_path = r\"R:\\projects\\vital_project\\live\\01NVa_Dengue\\RAW_DATA\"\n",
    "\n",
    "#Define clinical question and setup appropriate path (adults or children)\n",
    "event_type = 'severities'\n",
    "#event_type = 'ICU-FU'\n",
    "if event_type == 'severities':\n",
    "    cohort_of_interest = 'adults'\n",
    "    dataset_path = os.path.join(base_path, \"Adults\")\n",
    "elif event_type == 'ICU-FU':\n",
    "    cohort_of_interest = 'children'\n",
    "    dataset_path = os.path.join(base_path, \"Children\")\n",
    "else:\n",
    "    raise ValueError(\"Invalid choice of event type. Pipeline currently thoroughly tested only for ['ICU-FU', 'severities']\")\n",
    "\n",
    "#Singla sampling rate. Smartcare is 100. Only change if working with GE data.\n",
    "sampling_rate = 100\n",
    "\n",
    "# this refers to the length of the segments that we examine\n",
    "# when determining whether a segment is unusable or not\n",
    "# for reference: 525.6 seconds is the duration of STFT used\n",
    "unusable_segment_length_seconds = 52.56\n",
    "unusable_segment_length_samples = int(unusable_segment_length_seconds * sampling_rate)\n",
    "# used for obtaining the unusable segments, should always \n",
    "# be None (non-None values used only for testing)\n",
    "num_rows_to_read = None\n",
    "\n",
    "\n",
    "# only important for Shock/Reshock events for now this window refers to the \n",
    "# time period that we are interested in before and after an event occurrence\n",
    "window_size_minutes = 2\n",
    "window_size_samples = window_size_minutes * 60 * sampling_rate\n",
    "\n",
    "# only important when working with forecasting dengue shock\n",
    "pred_horizon_mins = 120\n",
    "pred_horizon      = pred_horizon_mins * 60 * sampling_rate\n",
    "\n",
    "# only important for RAW FE method (i.e, using raw signal as features)\n",
    "# this refers to the length of the segments that we cut the signal \n",
    "# up into to produce training samples in the feature extraction stage\n",
    "fe_segment_length_seconds = 30\n",
    "fe_segment_length_samples = int(fe_segment_length_seconds * sampling_rate)\n",
    "\n",
    "\n",
    "# choose which wavelength to use from the two that the Smartcare sensor collects: 'RED' or 'IR'\n",
    "signal_wavelength = 'RED'\n",
    "\n",
    "# choose which feature extraction method is applied\n",
    "# fe_method = 'RAW'\n",
    "# fe_method = 'FFT'\n",
    "fe_method = 'STFT'\n",
    "\n",
    "# this refers to whether we are using an STFT of the full possible length \n",
    "# (optimally 10 wavelengths of the lowest frequency of interest) or whether \n",
    "# we are using a smaller STFT to increase the number of training examples. \n",
    "# The STFT divider dictates how many times smaller the chosen STFT is then the \"optimal\" one\n",
    "window_divider = 1\n",
    "\n",
    "\n",
    "#    num_of_classes = 2\n",
    "num_of_classes = 2\n",
    "test_split = 0.2\n",
    "epochs = 100\n",
    "\n",
    "filter_params = {\n",
    "    'type' : 'cheby',\n",
    "    'order' : 2,\n",
    "    'sample_rate_Hz' : 100,\n",
    "    'low_cutoff_Hz' : 0.15,\n",
    "    'high_cutoff_Hz' : 20\n",
    "}\n",
    "\n",
    "sqi_thresholds = {\n",
    "    'MSQ' : 0.8,\n",
    "    'zero_cross' : [0.01, 0.04]\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create file names for the intermediate files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define pickle names and locations for the different stages of the pipeline\n",
    "patient_info_pickle_name = 'patient_info.pkl'\n",
    "patient_class_segments_pickle_name = 'segmented_classification_physio.pkl'\n",
    "unusable_data_pickle_name = f'unusable_data_{cohort_of_interest}_{signal_wavelength}_{unusable_segment_length_samples}_{num_rows_to_read}.pkl'\n",
    "patient_matrix_pickle_name = f'patient_matrix_{cohort_of_interest}_{signal_wavelength}_{event_type}_{window_size_samples}.pkl'\n",
    "\n",
    "patient_info_pickle_path = os.path.join(root_path, 'data_pickles', 'patient_info', patient_info_pickle_name)\n",
    "class_segment_pickle_path = os.path.join(root_path, 'data_pickles', 'patient_info', patient_class_segments_pickle_name)\n",
    "unusable_data_pickle_path = os.path.join(root_path, 'data_pickles', 'unusable_data', unusable_data_pickle_name)\n",
    "patient_matrix_pickle_path = os.path.join(root_path, 'data_pickles', 'patient_matrix', patient_matrix_pickle_name)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load patient information list\n",
    "\n",
    "In this step a previously created list of all files including clinical information is loaded to guide patient data extraction. \n",
    "The file is a python dictonary, storing details about all raw PPG files as well as clinical data.\n",
    "\n",
    "It is structured as follows:\n",
    "\n",
    "* PATIENT_ID\n",
    "    * List of GE recordings\n",
    "        * Filename\n",
    "        * Start time\n",
    "        * End time\n",
    "        * Duration\n",
    "    * List of SmartCare recordings\n",
    "        * Filename\n",
    "        * Start time\n",
    "        * End time\n",
    "        * Duration\n",
    "    * Vital sign readings\n",
    "    * Lab readings\n",
    "    * Fluid readings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load patient info pickle\n",
    "patient_info = load_pickle(patient_info_pickle_path)\n",
    "#print(patient_info)\n",
    "print_num_patients_with_SC(patient_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Switch for more advanced feature extraction based on segment classification\n",
    "#For none - old operation\n",
    "#segmented_labels = {} \n",
    "segmented_labels = load_pickle(class_segment_pickle_path)\n",
    "\n",
    "# print(segmented_labels)\n",
    "\n",
    "trial_lbl = segmented_labels['003-2200']\n",
    "#print(trial_lbl)\n",
    "print(len(segmented_labels))\n",
    "print(trial_lbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_vital = pd.read_excel(\"R:/projects/vital_project/live/01NVa_Dengue/CLINICAL/clinical_data_sheet_copy.xlsx\", sheet_name = 2)\n",
    "df_static  = pd.read_csv(root_path + \"/01NVa_clinical_data/output/merged_staticdata.csv\")\n",
    "df_cli     = pd.read_csv(root_path + \"/01NVa_clinical_data/output/cleaned_cli.csv\")\n",
    "df_reshock = pd.read_csv(root_path + \"/01NVa_clinical_data/output/cleaned_cli_reshock.csv\")\n",
    "df_vital   = pd.read_csv(root_path + \"/01NVa_clinical_data/output/cleaned_vital.csv\")\n",
    "df_dengue  = pd.read_csv(root_path + \"/01NVa_clinical_data/output/cleaned_dengue_.csv\")\n",
    "\n",
    "\n",
    "\n",
    "check_times = df_static[\"SUBJID\"]\n",
    "shock_adm   = df_cli[\"SUBJID\"]\n",
    "id_array    = df_dengue[\"id\"]\n",
    "\n",
    "for key in segmented_labels:\n",
    "    \n",
    "    if(len(segmented_labels[str(key)].keys()) != 0):\n",
    "        _key_str_ = str(key)\n",
    "        key_str = _key_str_[4:]\n",
    "        \n",
    "        print(key_str)\n",
    "        for item in segmented_labels[str(key)]:\n",
    "            shock_obs = []\n",
    "            timestamp_str = str(item)\n",
    "\n",
    "            # print(segmented_labels[str(key)][str(item)])\n",
    "            tmp_date = datetime.datetime.strptime(timestamp_str[:-4], \"%Y%m%dT%H%M%S.%f%z\")\n",
    "            start_time_ = time.mktime(tmp_date.timetuple())\n",
    "            print(tmp_date)\n",
    "            print(start_time_)\n",
    "\n",
    "            k_idx = np.where(check_times == int(key_str))[0]\n",
    "            m_idx = np.where(shock_adm == int(key_str))[0]\n",
    "            n_idx = np.where(id_array == int(key_str))[0]\n",
    "            print(k_idx)\n",
    "            print(m_idx)\n",
    "            print(n_idx)\n",
    "\n",
    "            str_datetime = df_static[\"adm_datetime\"][k_idx].values[0]\n",
    "            if(str_datetime!=\"-1\"):\n",
    "                ed_date = datetime.datetime.strptime(str_datetime+\"+0700\", \"%d/%m/%Y %H:%M%z\")\n",
    "                print(ed_date)\n",
    "\n",
    "                ed_start_time = time.mktime(ed_date.timetuple())\n",
    "                print(ed_start_time)\n",
    "\n",
    "                df = segmented_labels[str(key)][str(item)]\n",
    "                df = df.reset_index()\n",
    "\n",
    "                df['shock_event'] = 0\n",
    "                df['shock_start_idx'] = df['start_idx']\n",
    "                df['shock_end_idx']   = df['end_idx']\n",
    "\n",
    "                for n in n_idx:\n",
    "                    start_time = df[\"start_idx\"]/100 + start_time_\n",
    "                    end_time   = df[\"end_idx\"]/100 + start_time_\n",
    "\n",
    "                    event_time  = df_dengue[\"t.stop\"][n]\n",
    "                    event_value = df_dengue[\"event\"][n]\n",
    "\n",
    "                    if(event_value==1):\n",
    "                        event_time_start_unix = ed_start_time + event_time*3600\n",
    "                        event_time_end_unix = ed_start_time + event_time*3600 + (10800*2)\n",
    "                        \n",
    "                        # print(event_time_start_unix)\n",
    "                        # print(event_time_end_unix)\n",
    "                        # print(start_time)\n",
    "                        # print(end_time)\n",
    "                        \n",
    "                        print(\"shock_event_start\")\n",
    "                        selected_label_1 = df.loc[(start_time <= event_time_start_unix ) & (end_time >= event_time_start_unix)]\n",
    "                        shock_start_idx  = int(100*(event_time_start_unix - start_time_))\n",
    "                        print(selected_label_1)\n",
    "\n",
    "                        print(\"shock_event_end\")\n",
    "                        selected_label_2 = df.loc[(start_time <= event_time_end_unix ) & (end_time >= event_time_end_unix)]\n",
    "                        shock_end_idx    = int(100*(event_time_end_unix - start_time_))\n",
    "                        print(selected_label_2)\n",
    "\n",
    "\n",
    "                        if(len(selected_label_1) == 0 and len(selected_label_2) > 0):\n",
    "                            print(selected_label_2.index[0])\n",
    "                            idx = selected_label_2.index[0]\n",
    "\n",
    "                            if(idx + 1 != len(df)):\n",
    "                                df[\"shock_end_idx\"][idx] = shock_end_idx\n",
    "                                df[\"shock_start_idx\"][idx + 1] = shock_end_idx + 1\n",
    "\n",
    "                            if(idx > 0):\n",
    "                                for k in range(0, selected_label_2.index[0] + 1):\n",
    "                                    df[\"shock_event\"][k] = 1\n",
    "                                    \n",
    "                            else:\n",
    "                                df[\"shock_event\"][idx]   = 1\n",
    "                                # df[\"shock_end_idx\"][idx] = shock_end_idx\n",
    "                                # df[\"shock_start_idx\"][idx + 1] = shock_end_idx + 1\n",
    "\n",
    "                        elif(len(selected_label_1) > 0 and len(selected_label_2) == 0):\n",
    "                            print(selected_label_1.index[0])\n",
    "\n",
    "                            idx = selected_label_1.index[0]\n",
    "                            if(idx > 0):\n",
    "                                if(selected_label_1.index[0] != 0):\n",
    "                                    df[\"shock_end_idx\"][idx - 1] = shock_start_idx - 1\n",
    "                                    df[\"shock_start_idx\"][idx]   = shock_start_idx\n",
    "\n",
    "                                for k in range(idx, len(selected_label_1) + 1):\n",
    "                    \n",
    "                                    df[\"shock_event\"][k] = 1\n",
    "                                    # if(k == selected_label_1.index[0]):\n",
    "                                    #     df[\"shock_end_idx\"][k] = shock_end_idx\n",
    "                                    #     df[\"shock_start_idx\"][k + 1] = shock_end_idx + 1\n",
    "                            else:\n",
    "                                df[\"shock_event\"][idx]   = 1\n",
    "                                # df[\"shock_end_idx\"][idx - 1] = shock_start_idx\n",
    "                                # df[\"shock_start_idx\"][idx]   = shock_start_idx\n",
    "\n",
    "\n",
    "                        elif(len(selected_label_1) > 0 and len(selected_label_2) > 0):\n",
    "                            print(selected_label_1.index[0])\n",
    "                            print(selected_label_2.index[0])\n",
    "\n",
    "                            num_idx = selected_label_2.index[0] - selected_label_1.index[0] + 1\n",
    "\n",
    "                            if(selected_label_2.index[0] + 1 != len(df)):\n",
    "                                idx = selected_label_2.index[0]\n",
    "                                df[\"shock_end_idx\"][idx] = shock_end_idx\n",
    "                                df[\"shock_start_idx\"][idx + 1] = shock_end_idx + 1\n",
    "                            \n",
    "                            if(selected_label_1.index[0] != 0):\n",
    "                                idx = selected_label_1.index[0]\n",
    "                                df[\"shock_end_idx\"][idx] = shock_end_idx\n",
    "                                df[\"shock_start_idx\"][idx + 1] = shock_end_idx + 1\n",
    "\n",
    "                            for k in range(selected_label_1.index[0], selected_label_2.index[0] + 1):\n",
    "                                df[\"shock_event\"][k] = 1\n",
    "                                \n",
    "                \n",
    "                \n",
    "                    else:\n",
    "                        print(\"no synchronised input\")\n",
    "                \n",
    "                segmented_labels[str(key)][str(item)] = df.set_index('index')\n",
    "\n",
    "    else:\n",
    "        print(\"no PPG observations\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_lbl = segmented_labels['003-2012']\n",
    "#print(trial_lbl)\n",
    "print(len(segmented_labels))\n",
    "print(trial_lbl)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pickle generating functions\n",
    "\n",
    "From now on, functions are called in a pipeline manner and after each step the results are saved into a Python class-preserving file format called pickle (.pkl)\n",
    "\n",
    "The function always checks if pickle doesn't already exists and if it does it simply loads it instead of computing to save on subsequent runs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter files\n",
    "\n",
    "Iterate through all applicable files and determine which segments are bad quality. \n",
    "\n",
    "Output is a list containing patient ID, filename and list of starting indexes within the file where noisy segment was identified. \n",
    "\n",
    "Each segment is approximately 52 seconds long.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there already exists a pickle with the unusable data for the current parameters chosen\n",
    "# currently only accounts for cohort and the length of the segments to be examined for quality checking\n",
    "# if the pickle exists, load it, if not, generate it\n",
    "unusable_data = load_or_create_pickle(pickle_path=unusable_data_pickle_path, \n",
    "                                        function_to_generate_pickle=generate_unusable_data_lists,\n",
    "                                        base_path=dataset_path,\n",
    "                                        unusable_segment_length_samples=unusable_segment_length_samples, \n",
    "                                        filter_params=filter_params, \n",
    "                                        sqi_thresholds=sqi_thresholds, \n",
    "                                        unusable_data_pickle_path=unusable_data_pickle_path,\n",
    "                                        signal_wavelength=signal_wavelength,\n",
    "                                        num_rows_to_read=num_rows_to_read)\n",
    "\n",
    "#print((unusable_data))\n",
    "print(len(unusable_data)) #224"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate patient matrix\n",
    "\n",
    "Reduce the total number of files to only the relevant one for the clinical question investigated.\n",
    "\n",
    "In the case of severity classification, all adults files are kept anyway (as long as they have SmartCare data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there already exists a pickle with the patient matrix for the current parameters chosen\n",
    "# currently only accounts for cohort, event type, and the size of the window before and after the event\n",
    "# if the pickle exists, load it, if not, generate it\n",
    "reduced_patient_matrix = load_or_create_pickle(pickle_path=patient_matrix_pickle_path, \n",
    "                                                function_to_generate_pickle=generate_and_reduce_patient_matrix,\n",
    "                                                base_path=dataset_path,\n",
    "                                                patient_info=patient_info,\n",
    "                                                event_type=event_type,\n",
    "                                                window_size_samples=window_size_samples,\n",
    "                                                unusable_data=unusable_data,\n",
    "                                                unusable_segment_length_samples=unusable_segment_length_samples,\n",
    "                                                patient_matrix_pickle_path=patient_matrix_pickle_path)\n",
    "print('Reduced patient matrix:')\n",
    "print_patient_matrix(reduced_patient_matrix, event_type)\n",
    "print('\\n')\n",
    "\n",
    "# reduced_patient_matrix = reduced_patient_matrix[0:10]\n",
    "\n",
    "# print('Reduced patient matrix:')\n",
    "# print_patient_matrix(reduced_patient_matrix, event_type)\n",
    "# print('\\n')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare training dataset and define pickle name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How long the STFT windows are. Multiplies of ~66 seconds. \n",
    "\n",
    "num_stft_windows = 1\n",
    "#num_stft_windows = 15\n",
    "\n",
    "stft_resolution = 1\n",
    "# Resolution of 1 minute STFT window (number of columns for 66 second segment). 1=2columns, \n",
    "# Supply list of pts\n",
    "pt_list = [2053, 2001, 2002]\n",
    "reduced_patient_matrix_adjusted = extract_patients(reduced_patient_matrix, pt_list)\n",
    "#print(reduced_patient_matrix_adjusted)\n",
    "adjust_pt_list = False\n",
    "#Feature extraction pickle name (change to trigger feature extraction pipeline)\n",
    "\n",
    "if adjust_pt_list:\n",
    "    reduced_patient_matrix_input = reduced_patient_matrix_adjusted\n",
    "else:\n",
    "    reduced_patient_matrix_input = reduced_patient_matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate features\n",
    "\n",
    "Depending on the type of model and feature extraction method, this step creates feature-label pairs for supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define unique name for given set of FE parameters\n",
    "num_of_pts = len(set([row[0][0] for row in reduced_patient_matrix_input]))\n",
    "classifier = 'physio_score'\n",
    "training_data_pickle_name = f'feature_extraction_{event_type}_{fe_method}_{num_stft_windows}_{stft_resolution}_{classifier}_{pred_horizon_mins}_mt_1.pkl'\n",
    "training_data_pickle_path = os.path.join(root_path, 'data_pickles', 'feature_extraction', fe_method, training_data_pickle_name)\n",
    "\n",
    "print(training_data_pickle_path)\n",
    "\n",
    "# check if there already exists a pickle with the training data/features for the current parameters chosen\n",
    "# currently only accounts for cohort, window size, and the size of segments we cut up the signal for FE\n",
    "# if the pickle exists, load it, if not, generate it\n",
    "# 0:108    108:216   216:\n",
    "#reduced_patient_matrix = reduced_patient_matrix[0:2]\n",
    "feature_extraction_data = load_or_create_pickle(pickle_path=training_data_pickle_path,\n",
    "                                                function_to_generate_pickle=pipeline_blocks.core_pipeline_function_wrappers.generate_features,\n",
    "                                                base_path=dataset_path,\n",
    "                                                event_type=event_type,\n",
    "                                                reduced_patient_matrix=reduced_patient_matrix_input,\n",
    "                                                window_size_samples=window_size_samples,\n",
    "                                                filter_params=filter_params,\n",
    "                                                signal_wavelength=signal_wavelength,\n",
    "                                                fe_method=fe_method,\n",
    "                                                fe_segment_length_samples=fe_segment_length_samples,\n",
    "                                                window_divider=window_divider,\n",
    "                                                num_stft_windows=num_stft_windows,\n",
    "                                                stft_resolution=stft_resolution,\n",
    "                                                training_data_pickle_path=training_data_pickle_path,\n",
    "                                                segmented_labels=segmented_labels,\n",
    "                                                label_name=classifier)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training and evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('* Preparing datasets for model training...')\n",
    "inputs, cs_output, shock_output, patients_id_ = feature_extraction_data\n",
    "# inputs, cs_output, patients_id_ = feature_extraction_data\n",
    "\n",
    "if fe_method == 'STFT':\n",
    "    num_bins = 128\n",
    "    truncating_point = int(0.4 * len(inputs[0]))\n",
    "    num_ffts_in_stft = inputs[0].shape[1]\n",
    "    start_time = time.time()\n",
    "    inputs = [preprocess_stft(stft, num_bins, truncating_point, num_ffts_in_stft) for stft in inputs]\n",
    "    end_time = time.time()\n",
    "    print(f'Preprocessing STFT inputs took {end_time - start_time} seconds')\n",
    "    # inputs = np.asarray(inputs)\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"Invalid FE method chosen, CNNs are used only for STFT\")\n",
    "\n",
    "_data_ = {'input': inputs, 'score_output':cs_output, 'shock_output':shock_output, 'id':patients_id_}\n",
    "# _data_ = {'input': inputs, 'score_output':cs_output, 'id':patients_id_}\n",
    "df = pd.DataFrame(data=_data_)\n",
    "patients_id = pd.unique(df['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.loc[df['id'] == '2103']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate static clinical and demographic information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_data_path = os.path.join(root_path, '01NVa_clinical_data', 'output', 'merged_staticdata.csv')\n",
    "df_dengue   = pd.read_csv(static_data_path)\n",
    "\n",
    "features_id = [\"SUBJID\", \"AGE\", \"WEIGHT\", \"HEIGHT\", \"SEX\", \"HYPER\", \"DIABETE\", \"HEADACHE\", \"VOMIT\", \"DIARRHOEA\", \"ABDOPAIN\", \"SHOCKADM\"]\n",
    "\n",
    "df_static = df_dengue[features_id]\n",
    "\n",
    "sex_mapping  = {'M':0, 'F':1}\n",
    "bool_mapping = {'N':0, 'Y':1}\n",
    "\n",
    "\n",
    "df_static=df_static.replace(to_replace=\"N\",value=0)\n",
    "df_static=df_static.replace(to_replace=\"Y\",value=1)\n",
    "df_static=df_static.replace(to_replace=\"M\",value=0)\n",
    "df_static=df_static.replace(to_replace=\"F\",value=1)\n",
    "\n",
    "\n",
    "df_static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x = df_static[[\"AGE\", \"WEIGHT\", \"HEIGHT\"]]\n",
    "df_x = df_x.replace(-1, np.nan)\n",
    "\n",
    "transformer = MaxAbsScaler()\n",
    "df_x = transformer.fit_transform(df_x)\n",
    "\n",
    "df_static[\"AGE\"] = df_x[:,0]\n",
    "df_static[\"WEIGHT\"] = df_x[:,1]\n",
    "df_static[\"HEIGHT\"] = df_x[:,2]\n",
    "\n",
    "df_static = df_static.replace(np.nan, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating an array detailing the unique number of patients for creating the subsequent dataset\n",
    "ids = df_static[\"SUBJID\"].to_numpy()\n",
    "unique_static_ids = set(ids)\n",
    "\n",
    "unique_id = set([int(id__) for id__ in patients_id])\n",
    "print(sorted(unique_id))\n",
    "print(sorted(unique_id - unique_static_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a routine for creating the spectrogram image sequences \n",
    "seq_len = 10\n",
    "_unique_patient_ids_ = pd.unique(df['id'])\n",
    "arrX, arrY1, arrY2, arrID = [], [], [], []\n",
    "uniqueID = []\n",
    "for _id_ in _unique_patient_ids_:\n",
    "    _arr_   = df.loc[df['id'] == _id_]['input'].reset_index(drop=True)\n",
    "    _label_1_ = df.loc[df['id'] == _id_]['score_output'].reset_index(drop=True)\n",
    "    _label_2_ = df.loc[df['id'] == _id_]['shock_output'].reset_index(drop=True)\n",
    "    if(len(_arr_) >= seq_len):\n",
    "        arr = np.array([x for x in _arr_])\n",
    "        for k in range(seq_len, len(arr)): #1\n",
    "            arrX.append(arr[k-seq_len:k])\n",
    "            arrY1.append(_label_1_[k-1])\n",
    "            arrY2.append(_label_2_[k-1])\n",
    "            arrID.append(_id_)\n",
    "            \n",
    "arrX = np.array(arrX)\n",
    "arrY1 = np.array(arrY1)\n",
    "arrY2 = np.array(arrY2)\n",
    "arrID = np.array(arrID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#detailing the distribution of labels for both tasks\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "\n",
    "from sklearn.utils.multiclass import type_of_target\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "print(type_of_target(arrY1))\n",
    "print(type_of_target(arrY2))\n",
    "y1 = label_encoder.fit_transform(arrY1)\n",
    "y2 = label_encoder.fit_transform(arrY2)\n",
    "seed = np.random.randint(1,1000)\n",
    "\n",
    "print(\"Label distribution for binary clinical score\")\n",
    "_ = plt.hist(y1, bins='auto')  # arguments are passed to np.histogram\n",
    "plt.title(\"Histogram with 'auto' bins\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Label distribution for shock prediction\")\n",
    "_ = plt.hist(y2, bins='auto')  # arguments are passed to np.histogram\n",
    "plt.title(\"Histogram with 'auto' bins\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect and discard NaN entries\n",
    "if event_type == 'severities':\n",
    "    inputs_train_contain_nans = np.isnan(arrX).any()\n",
    "    if inputs_train_contain_nans:\n",
    "        print('Input data contains nan values, removing now...')\n",
    "        arrX1, arrY1, arrID1 = remove_nan_values_from_dataset(arrX, arrY1, arrID)\n",
    "        arrX2, arrY2, arrID2 = remove_nan_values_from_dataset(arrX, arrY2, arrID)\n",
    "        arrX  = arrX1\n",
    "        arrID = arrID1 \n",
    "    else:\n",
    "        print('Input data does_train not contain nan values, all good!')\n",
    "    print_class_balance(arrX, arrY1)\n",
    "    print_class_balance(arrX, arrY2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_static = df_static.drop_duplicates()\n",
    "patients_id = set(arrID)\n",
    "timestep = seq_len\n",
    "no_of_columns = len(df_static.columns) - 1\n",
    "static_array = []\n",
    "\n",
    "for k in range(len(arrID)):\n",
    "    print(arrID[k])\n",
    "    _static_query_ = df_static.loc[df_static['SUBJID'] == int(arrID[k])]\n",
    "    static_query_ = _static_query_.to_numpy()[0][1:]\n",
    "    static_query = np.broadcast_to(static_query_, (timestep,no_of_columns))\n",
    "    static_array.append(static_query)\n",
    "\n",
    "static_input_ = np.array(static_array)\n",
    "static_input  = static_input_.astype(float)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup model for training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestep = arrX[0].shape[0]\n",
    "spectrogram_height = arrX[0].shape[1]\n",
    "spectrogram_width = arrX[0].shape[2]\n",
    "input_dim_ = static_input[0].shape[1]\n",
    "output_dim_ = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn(input_shape, number_of_classes):\n",
    "\n",
    "    kernel_size = (4, 4)\n",
    "    max_pooling_dims = (2, 2)\n",
    "\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.InputLayer(input_shape=input_shape))\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(4, kernel_size, activation=tf.keras.layers.LeakyReLU(), padding='same'))\n",
    "    model.add(tf.keras.layers.Dropout(0.1))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(max_pooling_dims, strides=max_pooling_dims, padding='same'))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(8, kernel_size, activation=tf.keras.layers.LeakyReLU(), padding='same'))\n",
    "    model.add(tf.keras.layers.Dropout(0.1))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(max_pooling_dims, strides=max_pooling_dims, padding='same'))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(16, kernel_size, activation=tf.keras.layers.LeakyReLU(), padding='same'))\n",
    "    model.add(tf.keras.layers.Dropout(0.2))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(max_pooling_dims, strides=max_pooling_dims, padding='same'))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(32, kernel_size, activation=tf.keras.layers.LeakyReLU(), padding='same'))\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(max_pooling_dims, strides=max_pooling_dims, padding='same'))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(32, kernel_size, activation=tf.keras.layers.LeakyReLU(), padding='same'))\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(max_pooling_dims, strides=max_pooling_dims, padding='same'))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(32))\n",
    "    model.add(tf.keras.layers.Dropout(0.2))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Dense(32))\n",
    "    model.add(tf.keras.layers.Dropout(0.2))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(16))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(8))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    return model\n",
    "\n",
    "def build_cnnlstm(input_shape, number_of_classes):\n",
    "\n",
    "    kernel_size = (4, 4)\n",
    "    max_pooling_dims = (2, 2)\n",
    "\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.InputLayer(input_shape=input_shape))\n",
    "\n",
    "    model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Conv2D(4, kernel_size, activation=tf.keras.layers.LeakyReLU(), padding='same')))\n",
    "    model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Dropout(0.1)))\n",
    "    model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.MaxPooling2D(max_pooling_dims, strides=max_pooling_dims, padding='same')))\n",
    "    model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.BatchNormalization()))\n",
    "\n",
    "    model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Conv2D(8, kernel_size, activation=tf.keras.layers.LeakyReLU(), padding='same')))\n",
    "    model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Dropout(0.1)))\n",
    "    model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.MaxPooling2D(max_pooling_dims, strides=max_pooling_dims, padding='same')))\n",
    "    model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.BatchNormalization()))\n",
    "\n",
    "    model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Conv2D(16, kernel_size, activation=tf.keras.layers.LeakyReLU(), padding='same')))\n",
    "    model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Dropout(0.2)))\n",
    "    model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.MaxPooling2D(max_pooling_dims, strides=max_pooling_dims, padding='same')))\n",
    "    model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.BatchNormalization()))\n",
    "\n",
    "    model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Conv2D(32, kernel_size, activation=tf.keras.layers.LeakyReLU(), padding='same')))\n",
    "    model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.MaxPooling2D(max_pooling_dims, strides=max_pooling_dims, padding='same')))\n",
    "    model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.BatchNormalization()))\n",
    "\n",
    "    model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Conv2D(32, kernel_size, activation=tf.keras.layers.LeakyReLU(), padding='same')))\n",
    "    model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.MaxPooling2D(max_pooling_dims, strides=max_pooling_dims, padding='same')))\n",
    "    model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.BatchNormalization()))\n",
    "\n",
    "    model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Flatten()))\n",
    "    model.add(tf.keras.layers.LSTM(32))\n",
    "    model.add(tf.keras.layers.Dense(32))\n",
    "    model.add(tf.keras.layers.Dropout(0.2))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Dense(32))\n",
    "    model.add(tf.keras.layers.Dropout(0.2))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(16))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(8))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final averaged metrics + plot"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perfom K-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(prob_pos, y, threshold_cp=0):\n",
    "    \"\"\"\n",
    "    Takes predictions and compares with truth labels\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import seaborn as sns\n",
    "    from sklearn.metrics import roc_curve\n",
    "    from sklearn.metrics import precision_recall_curve, auc\n",
    "    from sklearn.metrics import precision_score\n",
    "    from sklearn.metrics import brier_score_loss\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.metrics import average_precision_score\n",
    "    fpr, tpr, thresholds = roc_curve(y, prob_pos)\n",
    "    J = tpr - fpr\n",
    "    if(threshold_cp!=0):\n",
    "        best_thresh = threshold_cp\n",
    "    else:\n",
    "        ix = np.argmax(J)\n",
    "        best_thresh = thresholds[ix]\n",
    "        #best_thresh = 0.5\n",
    "    print('Best Threshold=%f' % (best_thresh))\n",
    "    #Output metrics\n",
    "    precision, recall, prc_threshold = precision_recall_curve(y, prob_pos)\n",
    "    \n",
    "    from sklearn.metrics import classification_report\n",
    "    \n",
    "    pred_class = prob_pos > best_thresh\n",
    "    print(classification_report(y,pred_class))\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    sns.heatmap(confusion_matrix(y,pred_class), annot=True, fmt='g',cmap='Blues')\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y,pred_class).ravel()\n",
    "    specificity = tn/(tn+fp)\n",
    "\n",
    "    from sklearn.metrics import recall_score\n",
    "    from sklearn.metrics import make_scorer\n",
    "    specificity_score = make_scorer(recall_score, pos_label=0)\n",
    "    npv = make_scorer(precision_score,pos_label=0)\n",
    "\n",
    "    metrics = {'roc_auc':'roc_auc',\n",
    "            'specificity':specificity_score,\n",
    "            'recall':'recall',\n",
    "            'precision/PPV':'precision',\n",
    "            'accuracy':'accuracy',\n",
    "            'npv':npv}\n",
    "\n",
    "    bs=brier_score_loss(y, prob_pos)\n",
    "    print('Average precision score', average_precision_score(y, prob_pos))\n",
    "    print('F1 Score',f1_score(y,pred_class))\n",
    "    print('Specificity', specificity)\n",
    "    sensitivity = tp/(tp+fn)\n",
    "    print('Sensitivity', sensitivity)\n",
    "    print('ROC score', roc_auc_score(y, prob_pos))\n",
    "    print('PRC score', auc(recall, precision))\n",
    "\n",
    "    print('PPV',tp/(tp+fp))\n",
    "    print('NPV',tn/(fn+tn))\n",
    "    print('BS',bs)\n",
    "\n",
    "def individual_metrics(prob_pos, y):\n",
    "    import numpy as np\n",
    "    import seaborn as sns\n",
    "    from sklearn.metrics import roc_curve\n",
    "    from sklearn.metrics import precision_recall_curve, auc\n",
    "    from sklearn.metrics import precision_score, recall_score\n",
    "    from sklearn.metrics import brier_score_loss\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import average_precision_score\n",
    "    fpr, tpr, thresholds = roc_curve(y, prob_pos)\n",
    "    J = tpr - fpr\n",
    "    ix = np.argmax(J)\n",
    "    best_thresh = thresholds[ix]\n",
    "    \n",
    "    print('Best Threshold=%f' % (best_thresh))\n",
    "    pred_class = prob_pos>best_thresh\n",
    "    print(classification_report(y,pred_class))\n",
    "    #Output metrics\n",
    "    print('Average precision score', average_precision_score(y, prob_pos))\n",
    "    print('F1 Score', f1_score(y,pred_class))\n",
    "    print('Precision', precision_score(y,pred_class))\n",
    "    print('Recall', recall_score(y,pred_class))\n",
    "    # print('ROC score', roc_auc_score(y, prob_pos))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "\n",
    "from sklearn.utils.multiclass import type_of_target\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "print(type_of_target(arrY1))\n",
    "print(type_of_target(arrY2))\n",
    "y1 = label_encoder.fit_transform(arrY1)\n",
    "y2 = label_encoder.fit_transform(arrY2)\n",
    "seed = np.random.randint(1,1000)\n",
    "\n",
    "_ = plt.hist(y2, bins='auto')  # arguments are passed to np.histogram\n",
    "plt.title(\"Histogram with 'auto' bins\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(y1, bins='auto')  # arguments are passed to np.histogram\n",
    "plt.title(\"Histogram with 'auto' bins\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type      = 'CNN' #model type is either CNN or CNNLSTM\n",
    "experiment_num  = 1\n",
    "save_filepath   = root_path + \"/results/\"+str(pred_horizon_mins)+\"/experiment_\"+str(experiment_num)+\"/\"+model_type+\"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(1,6):\n",
    "    # idx = 5\n",
    "\n",
    "    train_idx_ = pd.read_csv(save_filepath + \"idx_set/train_idx_\"+str(idx)+\".csv\")\n",
    "    train_idx = np.squeeze(train_idx_.values)\n",
    "    train_idx = train_idx.astype(np.int64)\n",
    "\n",
    "    val_idx_ = pd.read_csv(save_filepath + \"idx_set/val_idx_\"+str(idx)+\".csv\")\n",
    "    val_idx = np.squeeze(val_idx_.values)\n",
    "    val_idx = val_idx.astype(np.int64)\n",
    "\n",
    "    test_idx_ = pd.read_csv(save_filepath + \"idx_set/test_idx_\"+str(idx)+\".csv\")\n",
    "    test_idx = np.squeeze(test_idx_.values)\n",
    "    test_idx = test_idx.astype(np.int64)\n",
    "\n",
    "    a = arrID[train_idx]\n",
    "    b = arrID[val_idx]\n",
    "    c = arrID[test_idx]\n",
    "\n",
    "\n",
    "    print('Training set')\n",
    "    display(len(set(a)))\n",
    "    print(set(a))\n",
    "    print('Size of training set',len(a))\n",
    "    print('Validation set')\n",
    "    display(len(set(b)))\n",
    "    print(sorted(set(b)-set(a)))\n",
    "    print(sorted(set(b)-set(c)))\n",
    "    print('Size of val set',len(b))\n",
    "    print('Test set')\n",
    "    display(len(set(c)))\n",
    "    print(sorted(set(c)-set(a)))\n",
    "    print('Size of test set',len(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 1\n",
    "\n",
    "cv= StratifiedGroupKFold(n_splits=8, shuffle=False, random_state=seed)\n",
    "for train_split, test_idx in cv.split(arrX, y2, arrID):\n",
    "    print(\"TRAIN:\", train_split, \"TEST:\", test_idx)\n",
    "\n",
    "X_train_main_ = arrX[train_split,:,:,:,:]\n",
    "X_train_aux_  = static_input[train_split,:,:]\n",
    "y_train_aux_  = y1[train_split]\n",
    "y_train_main_ = y2[train_split]\n",
    "train_pID     = arrID[train_split]\n",
    "\n",
    "#and now to split to train/val by repeating the above again but on the train_idx sets \n",
    "cv = StratifiedGroupKFold(n_splits=5, shuffle=False, random_state=seed)\n",
    "\n",
    "for train_idx, val_idx in cv.split(X_train_main_, y_train_aux_, train_pID):\n",
    "    print(\"TRAIN:\", train_idx, \"VAL:\", val_idx)\n",
    "\n",
    "    np.savetxt(save_filepath + \"idx_set/train_idx_\" + str(count) + \".csv\", train_split[train_idx], delimiter=\",\")\n",
    "    np.savetxt(save_filepath + \"idx_set/val_idx_\" + str(count) + \".csv\", train_split[val_idx], delimiter=\",\")\n",
    "    np.savetxt(save_filepath + \"idx_set/test_idx_\" + str(count) + \".csv\", test_idx, delimiter=\",\")\n",
    "\n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conformal prediction settings\n",
    "enable_cp = 0\n",
    "alpha = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "1. The 5 models are trained from the 5 CV folds and tested on the independent test set.\n",
    "2. At this stage, the imbalanced data in the dengue shock labels are randomly re-sampled in order to balance the dataset. \n",
    "3. [Optional] The application of conformal prediction to control the level of Type I errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model      = 1\n",
    "enable_mtl      = 1\n",
    "\n",
    "for idx in range(1,6):\n",
    "    # idx = 5\n",
    "\n",
    "    train_idx_ = pd.read_csv(save_filepath + \"idx_set/train_idx_\"+str(idx)+\".csv\")\n",
    "    train_idx = np.squeeze(train_idx_.values)\n",
    "    train_idx = train_idx.astype(np.int64)\n",
    "\n",
    "    val_idx_ = pd.read_csv(save_filepath + \"idx_set/val_idx_\"+str(idx)+\".csv\")\n",
    "    val_idx = np.squeeze(val_idx_.values)\n",
    "    val_idx = val_idx.astype(np.int64)\n",
    "\n",
    "    test_idx_ = pd.read_csv(save_filepath + \"idx_set/test_idx_\"+str(idx)+\".csv\")\n",
    "    test_idx = np.squeeze(test_idx_.values)\n",
    "    test_idx = test_idx.astype(np.int64)\n",
    "\n",
    "    a = arrID[train_idx]\n",
    "    b = arrID[val_idx]\n",
    "    c = arrID[test_idx]\n",
    "\n",
    "\n",
    "    print('Training set')\n",
    "    display(len(set(a)))\n",
    "    print('Size of training set',len(a))\n",
    "    print('Validation set')\n",
    "    display(len(set(b)))\n",
    "    print(sorted(set(b)-set(a)))\n",
    "    print(sorted(set(b)-set(c)))\n",
    "    print('Size of val set',len(b))\n",
    "    print('Test set')\n",
    "    display(len(set(c)))\n",
    "    print(sorted(set(c)-set(a)))\n",
    "    print('Size of test set',len(c))\n",
    "\n",
    "    #Confirm exclusivity of patient data in train/val/test sets\n",
    "    print(a.tolist() in b.tolist())\n",
    "    print(b.tolist() in c.tolist())\n",
    "\n",
    "    X_train_main = arrX[train_idx,:,:,:,:]\n",
    "    X_train_aux  = static_input[train_idx,:,:]\n",
    "    X_val_main   = arrX[val_idx,:,:,:,:]\n",
    "    X_val_aux    = static_input[val_idx,:,:]\n",
    "    X_test_main  = arrX[test_idx,:,:,:,:]\n",
    "    X_test_aux   = static_input[test_idx,:,:]\n",
    "\n",
    "    y_train_aux   = y1[train_idx]\n",
    "    y_train_main  = y2[train_idx]\n",
    "    y_val_aux     = y1[val_idx]\n",
    "    y_val_main    = y2[val_idx]\n",
    "    y_test_aux    = y1[test_idx]\n",
    "    y_test_main   = y2[test_idx]\n",
    "\n",
    "    bool_train_labels = y_train_main != 0\n",
    "\n",
    "\n",
    "    display(pd.DataFrame(y_train_main).value_counts(normalize=True))\n",
    "    display(pd.DataFrame(y_train_aux).value_counts(normalize=True))\n",
    "    display(pd.DataFrame(y_val_main).value_counts(normalize=True))\n",
    "    display(pd.DataFrame(y_val_aux).value_counts(normalize=True))\n",
    "    display(pd.DataFrame(y_test_main).value_counts(normalize=True))\n",
    "\n",
    "    main_pos_features = X_train_main[bool_train_labels]\n",
    "    main_neg_features = X_train_main[~bool_train_labels]\n",
    "    aux_pos_features = X_train_aux[bool_train_labels]\n",
    "    aux_neg_features = X_train_aux[~bool_train_labels]\n",
    "\n",
    "    pos_labels = y_train_main[bool_train_labels]\n",
    "    neg_labels = y_train_main[~bool_train_labels]\n",
    "\n",
    "    aux_pos_labels = y_train_aux[bool_train_labels]\n",
    "    aux_neg_labels = y_train_aux[~bool_train_labels]\n",
    "\n",
    "    ids = np.arange(len(main_pos_features))\n",
    "    choices = np.random.choice(ids, len(main_neg_features))\n",
    "\n",
    "    res_pos_features = main_pos_features[choices]\n",
    "    res_pos_labels   = pos_labels[choices]\n",
    "    aux_res_pos_features = aux_pos_features[choices]\n",
    "\n",
    "    aux_res_pos_labels   = aux_pos_labels[choices]\n",
    "\n",
    "\n",
    "    resampled_features = np.concatenate([res_pos_features, main_neg_features], axis=0)\n",
    "    resampled_labels = np.concatenate([res_pos_labels, neg_labels], axis=0)\n",
    "    aux_resampled_features = np.concatenate([aux_res_pos_features, aux_neg_features], axis=0)\n",
    "\n",
    "    aux_resampled_labels = np.concatenate([aux_res_pos_labels, aux_neg_labels], axis=0)\n",
    "\n",
    "    order = np.arange(len(resampled_labels))\n",
    "    np.random.shuffle(order)\n",
    "    resampled_features = resampled_features[order]\n",
    "    resampled_labels = resampled_labels[order]\n",
    "    aux_resampled_features = aux_resampled_features[order]\n",
    "\n",
    "    aux_resampled_labels = aux_resampled_labels[order]\n",
    "\n",
    "\n",
    "\n",
    "    if(load_model == 0):\n",
    "\n",
    "        if(model_type == 'CNN'):\n",
    "            tmp_x_train =  resampled_features[:,-1,:,:,:]\n",
    "            X_val_main  =  X_val_main[:,-1,:,:,:]\n",
    "            X_test_main =  X_test_main[:,-1,:,:,:]\n",
    "            # X_test_aux  =  X_test_aux[:,-1,:,:,:]\n",
    "\n",
    "            opt = tf.keras.optimizers.Adam(learning_rate=3e-4)\n",
    "            input_shape = tmp_x_train[0].shape\n",
    "            output_dim = 1\n",
    "\n",
    "            modelSFT = build_cnn(input_shape, output_dim)\n",
    "        else:\n",
    "            tmp_x_train =  resampled_features\n",
    "            X_val_main  =  X_val_main\n",
    "            X_test_main =  X_test_main\n",
    "            # X_test_aux  =  X_test_aux[:,-1,:,:,:]\n",
    "\n",
    "            opt = tf.keras.optimizers.Adam(learning_rate=3e-4)\n",
    "            input_shape = tmp_x_train[0].shape\n",
    "            output_dim = 1\n",
    "\n",
    "            modelSFT = build_cnnlstm(input_shape, output_dim)\n",
    "            \n",
    "        if(idx == 1):\n",
    "            print(modelSFT.summary())\n",
    "\n",
    "        modelSFT.compile(loss='binary_crossentropy',\n",
    "                          optimizer=opt,\n",
    "                          metrics=tf.keras.metrics.AUC(name='auc'))\n",
    "        \n",
    "        with tf.device('/cpu:0'):\n",
    "            history = modelSFT.fit(tmp_x_train, \n",
    "                                aux_resampled_labels, \n",
    "                                validation_data=(X_val_main, y_val_aux),   \n",
    "                                batch_size=32, \n",
    "                                epochs=30, \n",
    "                                callbacks=[tf.keras.callbacks.EarlyStopping('val_loss', patience=30, restore_best_weights=False)])\n",
    "        \n",
    "        modelSFT.save(save_filepath + \"models/modelSFT_\" + str(idx) + \".h5\")\n",
    "\n",
    "    else:\n",
    "        modelSFT = tf.keras.models.load_model(save_filepath + 'models/modelSFT_' + str(idx) + '.h5')\n",
    "\n",
    "    if(enable_cp):\n",
    "        cal_pred_shock = modelSFT.predict(X_val_main)\n",
    "\n",
    "        n = len(y_val_main)\n",
    "        normal = y_val_main == 0\n",
    "        c_preds_normal = cal_pred_shock[normal]\n",
    "        c_preds_shock  = cal_pred_shock[np.invert(normal)]\n",
    "\n",
    "        cal_scores = c_preds_normal\n",
    "\n",
    "        # Use the outlier detection method to get a threshold on the toxicities\n",
    "        qhat = np.quantile(cal_scores, np.ceil((n+1)*(1-alpha))/n)\n",
    "        \n",
    "\n",
    "        predictions_cs= modelSFT.predict(X_test_main)\n",
    "\n",
    "        normal = y_test_main == 0\n",
    "        preds_normal = predictions_cs[normal]\n",
    "        preds_shock  = predictions_cs[np.invert(normal)]\n",
    "        # Perform outlier detection on the ind and ood data\n",
    "        outlier_ind = preds_normal > qhat # We want this to be no more than alpha on average\n",
    "        outlier_ood = preds_shock > qhat # We want this to be as large as possible, but it doesn't have a guarantee\n",
    "\n",
    "        # Calculate type-1 and type-2 errors\n",
    "        type1 = outlier_ind.mean()\n",
    "        type2 = 1-outlier_ood.mean()\n",
    "        print(f\"The type-1 error is {type1:.4f}, the type-2 error is {type2:.4f}, and the threshold is {qhat:.4f}.\")\n",
    "\n",
    "        print(\"RESULT \" + str(idx))\n",
    "        # metrics(predictions_shock, y_test_main)\n",
    "        metrics(predictions_cs, y_test_aux, qhat)\n",
    "        for id in set(c):\n",
    "            __idx__ = np.where(c == id)[0]\n",
    "            print(str(id))\n",
    "            individual_metrics(predictions_cs[__idx__], y_test_aux[__idx__], qhat)\n",
    "    else:\n",
    "        predictions_cs = modelSFT.predict(X_test_main)\n",
    "\n",
    "        print(\"RESULT \" + str(idx))\n",
    "        # metrics(predictions_shock, y_test_aux)\n",
    "        metrics(predictions_cs, y_test_main)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dengue",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "40b238d7a1c4ca083cca694ef76ec3f2c499b33d774697c9f6fb3341f526e94a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
